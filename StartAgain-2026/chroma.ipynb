{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9afd1bd",
   "metadata": {},
   "source": [
    "# üß† End-to-End Semantic Search with ChromaDB, LangChain, and Embeddings\n",
    "\n",
    "## üìö Overview\n",
    "\n",
    "This notebook provides a **step-by-step guide** to building a semantic search pipeline using **ChromaDB** and **LangChain**.  \n",
    "You'll learn how to:\n",
    "\n",
    "- Load and preprocess text data\n",
    "- Split documents into manageable chunks\n",
    "- Generate vector embeddings using Ollama models\n",
    "- Store and index embeddings in ChromaDB\n",
    "- Perform fast similarity search over your data\n",
    "- Persist and reload your vector database for production use\n",
    "\n",
    "Whether you're building a **Retrieval-Augmented Generation (RAG)** system, a semantic search engine, or exploring vector databases, this notebook demonstrates all the core fundamentals you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4599b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\gen_ai_project\\.venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-chroma in d:\\gen_ai_project\\.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-community in d:\\gen_ai_project\\.venv\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: chromadb in d:\\gen_ai_project\\.venv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\gen_ai_project\\.venv\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain) (1.2.6)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.1)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\gen_ai_project\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.1)\n",
      "Requirement already satisfied: certifi in d:\\gen_ai_project\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\gen_ai_project\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\gen_ai_project\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-chroma) (2.4.0)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (0.22.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from chromadb) (4.26.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (2.3.0)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\gen_ai_project\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\gen_ai_project\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.47.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\gen_ai_project\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in d:\\gen_ai_project\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\gen_ai_project\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in d:\\gen_ai_project\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.2)\n",
      "Requirement already satisfied: sympy in d:\\gen_ai_project\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
      "Requirement already satisfied: filelock in d:\\gen_ai_project\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.12.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\gen_ai_project\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install all required dependencies for this notebook\n",
    "!pip install langchain langchain-chroma langchain-community chromadb python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7b9f0",
   "metadata": {},
   "source": [
    "# ü¶ô Setting Up Ollama for Embeddings\n",
    "\n",
    "To use **OllamaEmbeddings** in this notebook, you must have [Ollama](https://ollama.com/) installed and running on your computer, and the `nomic-embed-text` model downloaded.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Step-by-Step Installation Guide\n",
    "\n",
    "### 1. **Install Ollama**\n",
    "\n",
    "Choose your operating system and follow the instructions:\n",
    "\n",
    "- **Windows:**  \n",
    "  Download and install from [Ollama for Windows](https://ollama.com/download).\n",
    "\n",
    "- **macOS:**  \n",
    "  Download and install from [Ollama for macOS](https://ollama.com/download).\n",
    "\n",
    "- **Linux:**  \n",
    "  Open your terminal and run:\n",
    "  ```bash\n",
    "  curl -fsSL https://ollama.com/install.sh | sh\n",
    "  ```\n",
    "\n",
    "### 2. **Start the Ollama Service**\n",
    "\n",
    "After installing, you need to start the Ollama server so it can process embedding requests.  \n",
    "**Open a new terminal window** (Command Prompt, PowerShell, or Terminal) and run:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Leave this terminal window open and running in the background while you use the notebook.\n",
    "\n",
    "### 3. **Download the `nomic-embed-text` Model**\n",
    "\n",
    "In the **same terminal window** where you started Ollama, run:\n",
    "\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "This command downloads the embedding model required for this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Helpful Resources\n",
    "\n",
    "- [Ollama Documentation](https://ollama.com/docs)\n",
    "- [Ollama Models Library](https://ollama.com/library)\n",
    "\n",
    "---\n",
    "\n",
    "> **Note for Beginners:**  \n",
    "> - Always keep the Ollama server running in the background while working with embeddings in this notebook.\n",
    "> - If you close the terminal running `ollama serve`, embeddings will not work until you start it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c952f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21485c",
   "metadata": {},
   "source": [
    "# üìÑ TextLoader\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**TextLoader** is a utility from LangChain that allows you to **load and read text files** into your workflow as documents.  \n",
    "It is commonly used to ingest raw text data for further processing, such as splitting, embedding, or indexing.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Reads plain text files into LangChain document objects\n",
    "- Supports various file formats (with other loaders)\n",
    "- Easy integration with downstream LangChain components\n",
    "- Useful for preparing data for vector databases or LLM pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86fc9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='\\n    \"Large Language Models (LLMs) represent a significant advancement in artificial intelligence, \"\\n    \"enabling machines to comprehend and generate human language with remarkable accuracy. \"\\n    \"Trained on vast amounts of textual data, these models are capable of performing a wide range of natural language processing tasks, \"\\n    \"such as translation, summarization, question answering, and content creation. \"\\n    \"The core technology behind LLMs is deep learning, particularly transformer architectures, \"\\n    \"which allow the models to capture intricate patterns and relationships within language. \"\\n    \"Prominent examples of LLMs include OpenAI\\'s GPT series and Google\\'s BERT and LaMDA models. \"\\n    \"These systems have revolutionized the way we interact with technology, making it possible for computers to understand context, \"\\n    \"generate coherent responses, and assist users in various domains. \"\\n    \"As LLMs continue to evolve, they are expected to play an increasingly important role in fields ranging from education and healthcare to business and entertainment.\"\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ba0ba",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è RecursiveCharacterTextSplitter\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**RecursiveCharacterTextSplitter** is a tool for **splitting large documents into smaller chunks** based on character count.  \n",
    "This is essential for processing long texts with LLMs or vector databases, which often have input size limits.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Splits text into manageable chunks (e.g., 500 characters)\n",
    "- Supports overlap between chunks for context preservation\n",
    "- Handles various document types\n",
    "- Improves retrieval and embedding quality in downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af97d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8a0d5",
   "metadata": {},
   "source": [
    "# üß¨ OllamaEmbeddings\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**OllamaEmbeddings** is an embedding model integration for LangChain that generates **vector representations** (embeddings) from text.  \n",
    "These embeddings capture the semantic meaning of text, enabling similarity search and retrieval tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Converts text into high-dimensional vectors\n",
    "- Supports various models (e.g., `nomic-embed-text`)\n",
    "- Integrates with vector databases like ChromaDB\n",
    "- Essential for semantic search and RAG workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabf8ba",
   "metadata": {},
   "source": [
    "# üì¶ ChromaDB\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**ChromaDB** is an **open-source vector database** designed for storing, indexing, and querying **embeddings** generated from text, images, or other unstructured data.  \n",
    "It is widely used in **LLM-powered applications** such as Retrieval-Augmented Generation (RAG), semantic search, and question-answering systems.\n",
    "\n",
    "ChromaDB is known for being **lightweight, fast, and easy to use**, especially for local and small-to-medium scale AI projects.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Stores vector embeddings with metadata\n",
    "- Performs semantic similarity search\n",
    "- Works locally (no cloud dependency)\n",
    "- Integrates seamlessly with LangChain\n",
    "- Supports persistent storage\n",
    "- Optimized for AI and LLM workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a1bbc",
   "metadata": {},
   "source": [
    "# üóÉÔ∏è Chroma.from_documents\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "`Chroma.from_documents` is a method to **create a ChromaDB vector store** directly from a list of documents and their embeddings.  \n",
    "This enables fast and efficient similarity search over your data.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Indexes documents with their embeddings\n",
    "- Supports in-memory and persistent storage\n",
    "- Enables fast semantic search and retrieval\n",
    "- Integrates seamlessly with LangChain pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "736ab47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x267ec26f3e0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectordb = Chroma.from_documents(splits, embedding=embedding)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50a1ce",
   "metadata": {},
   "source": [
    "# üîé Similarity Search\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**Similarity search** is the process of finding documents in a vector database that are **most similar** to a given query, based on their embeddings.  \n",
    "This is a core technique in semantic search and retrieval-augmented generation (RAG).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Finds relevant documents using vector distance (e.g., cosine similarity)\n",
    "- Enables question-answering over large text corpora\n",
    "- Powers intelligent search in LLM applications\n",
    "- Fast and scalable with vector databases like ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0907d5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"The core technology behind LLMs is deep learning, particularly transformer architectures, \"\\n    \"which allow the models to capture intricate patterns and relationships within language. \"\\n    \"Prominent examples of LLMs include OpenAI\\'s GPT series and Google\\'s BERT and LaMDA models. \"\\n    \"These systems have revolutionized the way we interact with technology, making it possible for computers to understand context, \"\\n    \"generate coherent responses, and assist users in various domains. \"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Querying\n",
    "\n",
    "query = \"What is core technology behind LLMs ?\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dc8f8",
   "metadata": {},
   "source": [
    "# üíæ Persisting ChromaDB\n",
    "\n",
    "## üîç Definition\n",
    "\n",
    "**Persisting** a ChromaDB instance means saving the vector database to disk, so it can be **reloaded and reused** later without rebuilding the index.  \n",
    "This is crucial for production applications and large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Key Features\n",
    "\n",
    "- Saves vector data and metadata to a specified directory\n",
    "- Enables fast reloads and avoids recomputation\n",
    "- Supports long-term storage for AI workflows\n",
    "- Simple API: just set `persist_directory` when creating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bebad46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving to the disk\n",
    "\n",
    "vectordb = Chroma.from_documents(splits, embedding=embedding, persist_directory=\"./chroma.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
