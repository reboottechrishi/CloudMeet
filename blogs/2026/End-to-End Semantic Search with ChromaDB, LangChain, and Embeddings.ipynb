{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9afd1bd",
   "metadata": {},
   "source": [
    "# ðŸ§  End-to-End Semantic Search with ChromaDB, LangChain, and Embeddings\n",
    "\n",
    "## ðŸ“š Overview\n",
    "\n",
    "This notebook provides a **step-by-step guide** to building a semantic search pipeline using **ChromaDB** and **LangChain**.  \n",
    "You'll learn how to:\n",
    "\n",
    "- Load and preprocess text data\n",
    "- Split documents into manageable chunks\n",
    "- Generate vector embeddings using Ollama models\n",
    "- Store and index embeddings in ChromaDB\n",
    "- Perform fast similarity search over your data\n",
    "- Persist and reload your vector database for production use\n",
    "\n",
    "Whether you're building a **Retrieval-Augmented Generation (RAG)** system, a semantic search engine, or exploring vector databases, this notebook demonstrates all the core fundamentals you need"
   ]
  },
{
  "cell_type": "markdown",
  "id": "cb38d123",
  "metadata": {},
  "source": [
    "![AI Image Generator App Architecture](https://raw.githubusercontent.com/data-abhishek/AWS-Certified-Data-Analyst-Preparation/main/chromalabimage.png)\n"
  ]
}

,
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies for this notebook\n",
    "!pip install langchain langchain-chroma langchain-community chromadb python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a7b9f0",
   "metadata": {},
   "source": [
    "# ðŸ¦™ Setting Up Ollama for Embeddings\n",
    "\n",
    "To use **OllamaEmbeddings** in this notebook, you must have [Ollama](https://ollama.com/) installed and running on your computer, and the `nomic-embed-text` model downloaded.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Step-by-Step Installation Guide\n",
    "\n",
    "### 1. **Install Ollama**\n",
    "\n",
    "Choose your operating system and follow the instructions:\n",
    "\n",
    "- **Windows:**  \n",
    "  Download and install from [Ollama for Windows](https://ollama.com/download).\n",
    "\n",
    "- **macOS:**  \n",
    "  Download and install from [Ollama for macOS](https://ollama.com/download).\n",
    "\n",
    "- **Linux:**  \n",
    "  Open your terminal and run:\n",
    "  ```bash\n",
    "  curl -fsSL https://ollama.com/install.sh | sh\n",
    "  ```\n",
    "\n",
    "### 2. **Start the Ollama Service**\n",
    "\n",
    "After installing, you need to start the Ollama server so it can process embedding requests.  \n",
    "**Open a new terminal window** (Command Prompt, PowerShell, or Terminal) and run:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "Leave this terminal window open and running in the background while you use the notebook.\n",
    "\n",
    "### 3. **Download the `nomic-embed-text` Model**\n",
    "\n",
    "In the **same terminal window** where you started Ollama, run:\n",
    "\n",
    "```bash\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "This command downloads the embedding model required for this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Helpful Resources\n",
    "\n",
    "- [Ollama Documentation](https://ollama.com/docs)\n",
    "- [Ollama Models Library](https://ollama.com/library)\n",
    "\n",
    "---\n",
    "\n",
    "> **Note for Beginners:**  \n",
    "> - Always keep the Ollama server running in the background while working with embeddings in this notebook.\n",
    "> - If you close the terminal running `ollama serve`, embeddings will not work until you start it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c952f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21485c",
   "metadata": {},
   "source": [
    "# ðŸ“„ TextLoader\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**TextLoader** is a utility from LangChain that allows you to **load and read text files** into your workflow as documents.  \n",
    "It is commonly used to ingest raw text data for further processing, such as splitting, embedding, or indexing.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Reads plain text files into LangChain document objects\n",
    "- Supports various file formats (with other loaders)\n",
    "- Easy integration with downstream LangChain components\n",
    "- Useful for preparing data for vector databases or LLM pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86fc9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ba0ba",
   "metadata": {},
   "source": [
    "# âœ‚ï¸ RecursiveCharacterTextSplitter\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**RecursiveCharacterTextSplitter** is a tool for **splitting large documents into smaller chunks** based on character count.  \n",
    "This is essential for processing long texts with LLMs or vector databases, which often have input size limits.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Splits text into manageable chunks (e.g., 500 characters)\n",
    "- Supports overlap between chunks for context preservation\n",
    "- Handles various document types\n",
    "- Improves retrieval and embedding quality in downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8a0d5",
   "metadata": {},
   "source": [
    "# ðŸ§¬ OllamaEmbeddings\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**OllamaEmbeddings** is an embedding model integration for LangChain that generates **vector representations** (embeddings) from text.  \n",
    "These embeddings capture the semantic meaning of text, enabling similarity search and retrieval tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Converts text into high-dimensional vectors\n",
    "- Supports various models (e.g., `nomic-embed-text`)\n",
    "- Integrates with vector databases like ChromaDB\n",
    "- Essential for semantic search and RAG workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabf8ba",
   "metadata": {},
   "source": [
    "# ðŸ“¦ ChromaDB\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**ChromaDB** is an **open-source vector database** designed for storing, indexing, and querying **embeddings** generated from text, images, or other unstructured data.  \n",
    "It is widely used in **LLM-powered applications** such as Retrieval-Augmented Generation (RAG), semantic search, and question-answering systems.\n",
    "\n",
    "ChromaDB is known for being **lightweight, fast, and easy to use**, especially for local and small-to-medium scale AI projects.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Stores vector embeddings with metadata\n",
    "- Performs semantic similarity search\n",
    "- Works locally (no cloud dependency)\n",
    "- Integrates seamlessly with LangChain\n",
    "- Supports persistent storage\n",
    "- Optimized for AI and LLM workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a1bbc",
   "metadata": {},
   "source": [
    "# ðŸ—ƒï¸ Chroma.from_documents\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "`Chroma.from_documents` is a method to **create a ChromaDB vector store** directly from a list of documents and their embeddings.  \n",
    "This enables fast and efficient similarity search over your data.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Indexes documents with their embeddings\n",
    "- Supports in-memory and persistent storage\n",
    "- Enables fast semantic search and retrieval\n",
    "- Integrates seamlessly with LangChain pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ab47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectordb = Chroma.from_documents(splits, embedding=embedding)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50a1ce",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Similarity Search\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**Similarity search** is the process of finding documents in a vector database that are **most similar** to a given query, based on their embeddings.  \n",
    "This is a core technique in semantic search and retrieval-augmented generation (RAG).\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Finds relevant documents using vector distance (e.g., cosine similarity)\n",
    "- Enables question-answering over large text corpora\n",
    "- Powers intelligent search in LLM applications\n",
    "- Fast and scalable with vector databases like ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying\n",
    "query = \"What is core technology behind LLMs ?\"\n",
    "docs = vectordb.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772dc8f8",
   "metadata": {},
   "source": [
    "# ðŸ’¾ Persisting ChromaDB\n",
    "\n",
    "## ðŸ” Definition\n",
    "\n",
    "**Persisting** a ChromaDB instance means saving the vector database to disk, so it can be **reloaded and reused** later without rebuilding the index.  \n",
    "This is crucial for production applications and large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Key Features\n",
    "\n",
    "- Saves vector data and metadata to a specified directory\n",
    "- Enables fast reloads and avoids recomputation\n",
    "- Supports long-term storage for AI workflows\n",
    "- Simple API: just set `persist_directory` when creating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebad46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to the disk\n",
    "vectordb = Chroma.from_documents(splits, embedding=embedding, persist_directory=\"./chroma.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa_appendix_1",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Appendix: Q&A (ChromaDB + LangChain + Ollama)\n",
    "\n",
    "### Q1. What is the main goal of this notebook?\n",
    "**Answer:** To build an end-to-end semantic search pipeline using LangChain, Ollama embeddings, and ChromaDB, including loading text, splitting, embedding, indexing, searching, and persisting the vector store.\n",
    "\n",
    "---\n",
    "### Q2. Which Python packages are installed at the beginning?\n",
    "**Answer:** langchain, langchain-chroma, langchain-community, chromadb, and python-dotenv.\n",
    "\n",
    "---\n",
    "### Q3. Why is Ollama required in this workflow?\n",
    "**Answer:** It provides the local embedding model (e.g., `nomic-embed-text`) used by OllamaEmbeddings to convert text into vectors.\n",
    "\n",
    "---\n",
    "### Q4. How do you start the Ollama server?\n",
    "**Answer:** Run `ollama serve` in a terminal and keep it running in the background.\n",
    "\n",
    "---\n",
    "### Q5. How do you download the `nomic-embed-text` model for embeddings?\n",
    "**Answer:** Execute `ollama pull nomic-embed-text` in the same terminal where Ollama is running.\n",
    "\n",
    "---\n",
    "### Q6. What does TextLoader do in this notebook?\n",
    "**Answer:** It reads the file `speech.txt` and returns a list of LangChain `Document` objects for downstream processing.\n",
    "\n",
    "---\n",
    "### Q7. What Python objects does `loader.load()` return?\n",
    "**Answer:** A list of `Document` objects, each with `page_content` and `metadata`.\n",
    "\n",
    "---\n",
    "### Q8. Why do we split documents into chunks before embedding?\n",
    "**Answer:** To respect model/context size limits and improve retrieval quality by indexing smaller, focused pieces of text.\n",
    "\n",
    "---\n",
    "### Q9. Which splitter is used and with what parameters?\n",
    "**Answer:** `RecursiveCharacterTextSplitter` with `chunk_size=500` and `chunk_overlap=0`.\n",
    "\n",
    "---\n",
    "### Q10. What does `OllamaEmbeddings(model=\"nomic-embed-text\")` do?\n",
    "**Answer:** It configures LangChain to generate embeddings locally using the `nomic-embed-text` model hosted by Ollama.\n",
    "\n",
    "---\n",
    "### Q11. What is the purpose of `Chroma.from_documents(...)`?\n",
    "**Answer:** It creates a ChromaDB vector store by computing embeddings for the provided documents and indexing them for similarity search.\n",
    "\n",
    "---\n",
    "### Q12. What does `vectordb.similarity_search(query)` return?\n",
    "**Answer:** A list of the most relevant `Document` objects to the query, typically ordered by cosine similarity score.\n",
    "\n",
    "---\n",
    "### Q13. In the example, what does `docs[0].page_content` represent?\n",
    "**Answer:** The text content of the top-most similar chunk retrieved from the vector store for the given query.\n",
    "\n",
    "---\n",
    "### Q14. How do you persist the ChromaDB index to disk?\n",
    "**Answer:** Provide a `persist_directory` when creating the store, e.g., `Chroma.from_documents(..., persist_directory=\"./chroma.db\")`.\n",
    "\n",
    "---\n",
    "### Q15. How can you reload a previously persisted ChromaDB store?\n",
    "**Answer:** Initialize it with the same directory and embedding function, e.g., `Chroma(persist_directory=\"./chroma.db\", embedding_function=embedding)`.\n",
    "\n",
    "---\n",
    "### Q16. What happens if `ollama serve` is not running when generating embeddings?\n",
    "**Answer:** Embedding requests will fail (connection/refused errors) because the local embedding server is unreachable.\n",
    "\n",
    "---\n",
    "### Q17. Why might you set a `chunk_overlap > 0`?\n",
    "**Answer:** To preserve context that spans chunk boundaries, which can improve retrieval for long sentences or cross-paragraph references.\n",
    "\n",
    "---\n",
    "### Q18. Which similarity metric is commonly used by Chroma for semantic search?\n",
    "**Answer:** Cosine similarity (as implied by the similarity search description).\n",
    "\n",
    "---\n",
    "### Q19. How do you change the number of results returned by similarity search?\n",
    "**Answer:** Pass `k`, e.g., `vectordb.similarity_search(query, k=5)`.\n",
    "\n",
    "---\n",
    "### Q20. Does ChromaDB require a cloud service to run?\n",
    "**Answer:** No. It works locally and is well-suited for small-to-medium projects without cloud dependencies.\n",
    "\n",
    "---\n",
    "### Q21. What is RAG and how does this pipeline support it?\n",
    "**Answer:** Retrieval-Augmented Generation combines retrieval (via Chroma similarity search) with generation (LLMs). This notebook builds the retrieval layer needed for RAG.\n",
    "\n",
    "---\n",
    "### Q22. How is metadata used in ChromaDB?\n",
    "**Answer:** Chroma stores embeddings alongside metadata (e.g., file name, chunk ID), enabling filtering and better traceability of results.\n",
    "\n",
    "---\n",
    "### Q23. Can you filter results by metadata during search?\n",
    "**Answer:** Yes, you can supply a `filter` dict to the search call (supported by Chroma via LangChain) to restrict matches based on metadata fields.\n",
    "\n",
    "---\n",
    "### Q24. How can you reset or recreate the vector database from scratch?\n",
    "**Answer:** Delete the `persist_directory` (e.g., `./chroma.db`) or specify a new directory/collection and rebuild from your documents.\n",
    "\n",
    "---\n",
    "### Q25. Why is `python-dotenv` included in the install even if not shown in code?\n",
    "**Answer:** It's commonly used to load environment variables (e.g., API keys) in notebooks and is often included for convenience.\n",
    "\n",
    "---\n",
    "### Q26. Which input file is ingested by this example pipeline?\n",
    "**Answer:** The plain text file `speech.txt`.\n",
    "\n",
    "---\n",
    "### Q27. What example query is demonstrated in the notebook?\n",
    "**Answer:** \"What is core technology behind LLMs ?\"\n",
    "\n",
    "---\n",
    "### Q28. How can you improve retrieval quality beyond character-based splitting?\n",
    "**Answer:** Consider semantic or token-based splitters, add overlap, tune chunk sizes, and enrich metadata.\n",
    "\n",
    "---\n",
    "### Q29. How do you ensure results are reproducible across runs?\n",
    "**Answer:** Persist the DB, keep chunking parameters constant, use the same embedding model, and maintain the same pre-processing pipeline.\n",
    "\n",
    "---\n",
    "### Q30. What Python version does the notebook metadata indicate?\n",
    "**Answer:** Python 3.12.5 (as listed in the notebook's language_info metadata).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
